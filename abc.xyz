
import pandas as pd
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain_community.llms import LLMRequests  # or replace with your LangChain-compatible LLM
from tqdm import tqdm

# Load your model (replace with actual implementation if not using LLMRequests)
llm = LLMRequests(
    url="http://localhost:11434/api/generate",  # Example for Ollama with Gemma
    model="gemma:3b",
    headers={"Content-Type": "application/json"},
)

# Define prompt
prompt_template = PromptTemplate.from_template("""
You are an expert evaluator reviewing two notes that comment on the same building defect.

Defect: {condition}
AI-Generated Note: {ai_note}
Human-Generated Note: {human_note}

Both notes refer to the same defect. Your task is to assess whether the AI and human notes:
- Convey the same observation or judgment,
- Align in meaning or implication,
- Contradict each other,
- Or diverge in interpretation or focus.

Also consider cases where:
- One or both notes state the defect does not exist (e.g., "None", "NA", "Not present").

Respond in the following format:
Alignment: <One of ["Exact", "Implied Agreement", "Partial Agreement", "Contradictory", "No Information", "Does Not Exist"]>
Reason: <Concise explanation>
""")

chain: Runnable = prompt_template | llm

def evaluate_notes_from_csv(csv_path: str, save_path: str = "results.csv"):
    df = pd.read_csv(csv_path)
    results = []

    for _, row in tqdm(df.iterrows(), total=len(df)):
        input_dict = {
            "condition": row.get("condition_name", ""),
            "ai_note": row.get("ai_note", ""),
            "human_note": row.get("human_note", "")
        }

        output = chain.invoke(input_dict)
        alignment_line = next((line for line in output.split("\n") if "Alignment:" in line), None)
        verdict = alignment_line.replace("Alignment:", "").strip() if alignment_line else "Unknown"

        result = {
            **row,
            "model_verdict": verdict
        }

        if "ground_truth" in df.columns:
            result["is_correct"] = verdict.lower() == str(row["ground_truth"]).strip().lower()

        results.append(result)

    result_df = pd.DataFrame(results)
    
    if "is_correct" in result_df.columns:
        accuracy = result_df["is_correct"].mean()
        print(f"\nâœ… Accuracy: {accuracy * 100:.2f}%")

    result_df.to_csv(save_path, index=False)
    print(f"Results saved to {save_path}")


# Example usage
# evaluate_notes_from_csv("your_file.csv", "output_file.csv")
